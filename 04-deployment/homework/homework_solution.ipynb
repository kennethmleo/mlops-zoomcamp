{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27205881",
   "metadata": {},
   "source": [
    "# Homework 4: batch processing of the Taxi Dataset\n",
    "\n",
    "In this homework, we'll deploy the ride duration model in batch mode. Like in homework 1, we'll use the Yellow Taxi Trip Records dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c51efaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn==1.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef880a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator DictVectorizer from version 1.5.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LinearRegression from version 1.5.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "with open('model.bin', 'rb') as f_in:\n",
    "    dv, model = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41c08294",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['PULocationID', 'DOLocationID']\n",
    "\n",
    "def read_data(filename):\n",
    "    df = pd.read_parquet(filename)\n",
    "    \n",
    "    df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "    df['duration'] = df.duration.dt.total_seconds() / 60\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
    "\n",
    "    df[categorical] = df[categorical].fillna(-1).astype('int').astype('str')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4854399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2023\n",
    "month = 3\n",
    "df = read_data(f'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year:04d}-{month:02d}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "669fda0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = df[categorical].to_dict(orient='records')\n",
    "X_val = dv.transform(dicts)\n",
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5873e56c",
   "metadata": {},
   "source": [
    "**Question 1:** What's the standard deviation of the predicted duration for this (March 2023) dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf532ae7-1897-428c-ba0c-875ccaf7d76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of predictions: 6.2475\n"
     ]
    }
   ],
   "source": [
    "std_pred = pd.Series(y_pred).std()\n",
    "print(f\"Standard deviation of predictions: {std_pred:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8044ce40",
   "metadata": {},
   "source": [
    "## Q2. Preparing the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e1df7d",
   "metadata": {},
   "source": [
    "Like in the course videos, we want to prepare the dataframe with the output. \n",
    "\n",
    "First, let's create an artificial `ride_id` column:\n",
    "\n",
    "```python\n",
    "df['ride_id'] = f'{year:04d}/{month:02d}_' + df.index.astype('str')\n",
    "```\n",
    "\n",
    "Next, write the ride id and the predictions to a dataframe with results. \n",
    "\n",
    "Save it as parquet:\n",
    "\n",
    "```python\n",
    "df_result.to_parquet(\n",
    "    output_file,\n",
    "    engine='pyarrow',\n",
    "    compression=None,\n",
    "    index=False\n",
    ")\n",
    "```\n",
    "\n",
    "What's the size of the output file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfce114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ride_id'] = f'{year:04d}/{month:02d}_' + df.index.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e66f904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame({'ride_id': df['ride_id'], 'y_pred': y_pred})\n",
    "\n",
    "output_file = 'output.parquet'\n",
    "\n",
    "df_result.to_parquet(\n",
    "    output_file,\n",
    "    engine='pyarrow',\n",
    "    compression=None,\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "762074a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.4610595703125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_size_bytes = os.path.getsize(output_file)\n",
    "file_size_mb = file_size_bytes / (1024 ** 2)\n",
    "file_size_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f065179",
   "metadata": {},
   "source": [
    "## Q3. Creating the scoring script\n",
    "\n",
    "Now let's turn the notebook into a script. \n",
    "\n",
    "Which command you need to execute for that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e9e005",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "jupyter nbconvert --to script notebook_name.ipynb\n",
    "```\n",
    "\n",
    "The resulting script was further transformed based on the remaining part of the homework. You can find the complete script at `batch_processing.py`\n",
    "\n",
    "As mentioned before, pipenv was used to install all the libraries by running\n",
    "\n",
    "```bash\n",
    "pip install pipenv\n",
    "pipenv --python 3.10.13 # initialize the environment\n",
    "pipenv install pyarrow==16.1.0 scikit-learn==1.5.0 pandas==2.2.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c790f902",
   "metadata": {},
   "source": [
    "**Question 4:** After installing the libraries, pipenv creates two files: `Pipfile` and `Pipfile.lock`. The `Pipfile.lock` file keeps the hashes of the dependencies we use for the virtual env. What's the first hash for the Scikit-Learn dependency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17fe75e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Pipfile.lock'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPipfile.lock\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      6\u001b[0m scikit_learn_hashes \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscikit-learn\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhashes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Pipfile.lock'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('Pipfile.lock') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "scikit_learn_hashes = data['default']['scikit-learn']['hashes']\n",
    "first_scikit_learn_hash = scikit_learn_hashes[0]\n",
    "\n",
    "print(f\"The first hash of scikit-learn is: {first_scikit_learn_hash}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ecfef7",
   "metadata": {},
   "source": [
    "The script was parametrized using `Click`. It accepts the following parameters:\n",
    "\n",
    "+ `--year`: Enter a year\n",
    "+ `--month`: Enter a month (1-12)\n",
    "+ `--metrics`: Print metrics about the predictions\n",
    "+ `--bucket`: Name of the S3 bucket to upload the results\n",
    "\n",
    "**Question 5:** Run the script for April 2023. What's the mean predicted duration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232a9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-14 11:20:00,974 - INFO - Starting data processing for 2023-04\n",
      "2024-06-14 11:20:00,974 - INFO - Loading and processing data...\n",
      "2024-06-14 11:20:12,420 - INFO - Data loaded successfully. Proceeding with predictions.\n",
      "2024-06-14 11:20:12,421 - INFO - Transforming categorical variables...\n",
      "2024-06-14 11:20:19,048 - INFO - Generating predictions using the model...\n",
      "2024-06-14 11:20:19,057 - INFO - Predictions generated successfully.\n",
      "2024-06-14 11:20:19,057 - INFO - Calculating prediction metrics...\n",
      "2024-06-14 11:20:19,065 - INFO - Prediction metrics - Mean: 14.292283, Std: 6.353997, Min: -16.328844, Max: 70.047721\n",
      "2024-06-14 11:20:19,611 - INFO - Preparing results for export...\n",
      "2024-06-14 11:20:19,631 - INFO - Exporting results to output.parquet...\n",
      "2024-06-14 11:20:19,896 - INFO - Data processing and export completed successfully.\n"
     ]
    }
   ],
   "source": [
    "! python3 batch_processing.py --year 2023 --month 4 --metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e5451",
   "metadata": {},
   "source": [
    "Finally, we'll package the script in the docker container. For that, we'll a base image that was prepared by the course tutor: [agrigorev/zoomcamp-model:mlops-2024-3.10.13-slim](https://hub.docker.com/layers/agrigorev/zoomcamp-model/mlops-2024-3.10.13-slim/images/sha256-f54535b73a8c3ef91967d5588de57d4e251b22addcbbfb6e71304a91c1c7027f?context=repo)\n",
    "\n",
    "This is what the content of this image is:\n",
    "\n",
    "```DockerFile\n",
    "FROM python:3.10.13-slim\n",
    "\n",
    "WORKDIR /app\n",
    "COPY [ \"model2.bin\", \"model.bin\" ]\n",
    "```\n",
    "\n",
    "This image already has a pickle file with a dictionary vectorizer and a model, so we use `agrigorev`'s image as base and we don't include our `model.bin` into the container. We build this container by running\n",
    "\n",
    "```bash\n",
    "docker build -t homework-4 .\n",
    "```\n",
    "\n",
    "**Question 6:** Now run the script with docker. What's the mean predicted duration for May 2023?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdee2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-14 16:20:26,007 - INFO - Starting data processing for 2023-05\n",
      "2024-06-14 16:20:26,008 - INFO - Loading and processing data...\n",
      "2024-06-14 16:20:38,461 - INFO - Data loaded successfully. Proceeding with predictions.\n",
      "2024-06-14 16:20:38,466 - INFO - Transforming categorical variables...\n",
      "2024-06-14 16:20:52,329 - INFO - Generating predictions using the model...\n",
      "2024-06-14 16:20:52,342 - INFO - Predictions generated successfully.\n",
      "2024-06-14 16:20:52,342 - INFO - Calculating prediction metrics...\n",
      "2024-06-14 16:20:52,351 - INFO - Prediction metrics - Mean: 0.191744, Std: 1.388140, Min: -5.206588, Max: 5.559183\n",
      "2024-06-14 16:20:53,256 - INFO - Preparing results for export...\n",
      "2024-06-14 16:20:53,272 - INFO - Exporting results to output.parquet...\n",
      "2024-06-14 16:20:53,674 - INFO - Data processing and export completed successfully.\n"
     ]
    }
   ],
   "source": [
    "! docker run --platform linux/amd64 -it homework-4 --year 2023 --month 5 --metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e468ca3",
   "metadata": {},
   "source": [
    "## Bonus: upload the result the cloud\n",
    "\n",
    "To achieve this part of the homework, we now create, in AWS:\n",
    "\n",
    "- An S3 bucket called `mlops-zoomcamp-fustincho`.\n",
    "- An IAM User and create access keys for it.\n",
    "- A policy that is attached directly to the user, with the following permissions:\n",
    "\n",
    "```json\n",
    "{\n",
    "\t\"Version\": \"2012-10-17\",\n",
    "\t\"Statement\": [\n",
    "\t\t{\n",
    "\t\t\t\"Effect\": \"Allow\",\n",
    "\t\t\t\"Action\": \"s3:ListBucket\",\n",
    "\t\t\t\"Resource\": \"arn:aws:s3:::mlops-zoomcamp-fustincho\"\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"Effect\": \"Allow\",\n",
    "\t\t\t\"Action\": \"s3:PutObject\",\n",
    "\t\t\t\"Resource\": \"arn:aws:s3:::mlops-zoomcamp-fustincho/*\"\n",
    "\t\t}\n",
    "\t]\n",
    "}\n",
    "```\n",
    "\n",
    "When `batch_processing.py` receives a bucket name as the `--bucket` parameter, then it attempts to upload, via the boto3 client, the resulting output. For this to work, it is imperative for `boto3` to find the AWS credentials. For this, we create an `env` file and paste the access keys we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66acafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .env\n"
     ]
    }
   ],
   "source": [
    "%%writefile .env\n",
    "AWS_ACCESS_KEY_ID=your-aws-access-key-id\n",
    "AWS_SECRET_ACCESS_KEY=your-aws-secret-access-key\n",
    "AWS_DEFAULT_REGION=your-region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f37bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-14 16:29:26,910 - INFO - Starting data processing for 2023-03\n",
      "2024-06-14 16:29:26,910 - INFO - Loading and processing data...\n",
      "2024-06-14 16:29:39,709 - INFO - Data loaded successfully. Proceeding with predictions.\n",
      "2024-06-14 16:29:39,710 - INFO - Transforming categorical variables...\n",
      "2024-06-14 16:29:51,171 - INFO - Generating predictions using the model...\n",
      "2024-06-14 16:29:51,183 - INFO - Predictions generated successfully.\n",
      "2024-06-14 16:29:51,183 - INFO - Calculating prediction metrics...\n",
      "2024-06-14 16:29:51,192 - INFO - Prediction metrics - Mean: 0.188769, Std: 1.391873, Min: -5.299526, Max: 5.559183\n",
      "2024-06-14 16:29:52,092 - INFO - Preparing results for export...\n",
      "2024-06-14 16:29:52,111 - INFO - Exporting results to output.parquet...\n",
      "2024-06-14 16:29:52,512 - INFO - Data processing and export completed successfully.\n",
      "2024-06-14 16:29:52,512 - INFO - Uploading results to S3 bucket: mlops-zoomcamp-fustincho\n",
      "2024-06-14 16:29:52,534 - INFO - Found credentials in environment variables.\n",
      "2024-06-14 16:30:13,588 - INFO - File uploaded successfully to S3 at 2023/03/output.parquet.\n"
     ]
    }
   ],
   "source": [
    "!docker run --platform linux/amd64 --env-file .env homework-4 --year 2023 --month 3 --metrics --bucket mlops-zoomcamp-fustincho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbeae0c",
   "metadata": {},
   "source": [
    "![](./img/s3_uploaded.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970f4e8",
   "metadata": {},
   "source": [
    "## Bonus: use Mage for batch inference\n",
    "\n",
    "The `../bonus-homework` folder contains the solution for this bonus homework, using the same model we used for the script. To run the mage project we use `docker compose up`. In Mage, we create a pipeline and separate parts of the `batch_processing.py` in blocks:\n",
    "\n",
    "![](./img/pipeline.png)\n",
    "\n",
    "The `year` and `month` were also added as global variables within the pipeline.\n",
    "\n",
    "Then we create a trigger that we will use to execute the pipeline:\n",
    "\n",
    "![](./img/trigger.png)\n",
    "\n",
    "With this API endpoint active, it is possible to trigger a batch processing job by running\n",
    "\n",
    "```bash\n",
    "curl -X POST http://localhost:6789/api/pipeline_schedules/1/pipeline_runs/5a78a51f345442b0b5ebee3cd941cd0f \\\n",
    "  --header 'Content-Type: application/json' \\\n",
    "  --data '\n",
    "{\n",
    "  \"pipeline_run\": {\n",
    "    \"variables\": {\n",
    "      \"year\": \"2023\",\n",
    "      \"month\": \"05\"\n",
    "    }\n",
    "  }\n",
    "}'\n",
    "```\n",
    "\n",
    "We will now test this using the `requests` module. We will predict the trip duration for May and June 2023:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74676ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'pipeline_run': {'id': 5, 'created_at': '2024-06-14 16:40:58', 'updated_at': '2024-06-14 16:40:58', 'pipeline_schedule_id': 1, 'pipeline_uuid': 'batch_process_taxi_dataset', 'execution_date': '2024-06-14 16:40:58.983406', 'status': 'initial', 'started_at': None, 'completed_at': None, 'variables': {'year': '2023', 'month': '05', 'execution_partition': '1/20240614T164058_983406'}, 'passed_sla': False, 'event_variables': {}, 'metrics': None, 'backfill_id': None, 'executor_type': 'local_python'}}\n",
      "200\n",
      "{'pipeline_run': {'id': 6, 'created_at': '2024-06-14 16:40:59', 'updated_at': '2024-06-14 16:40:59', 'pipeline_schedule_id': 1, 'pipeline_uuid': 'batch_process_taxi_dataset', 'execution_date': '2024-06-14 16:40:59.037369', 'status': 'initial', 'started_at': None, 'completed_at': None, 'variables': {'year': '2023', 'month': '06', 'execution_partition': '1/20240614T164059_037369'}, 'passed_sla': False, 'event_variables': {}, 'metrics': None, 'backfill_id': None, 'executor_type': 'local_python'}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:6789/api/pipeline_schedules/1/pipeline_runs/5a78a51f345442b0b5ebee3cd941cd0f\"\n",
    "headers = {\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "data = {\n",
    "    \"pipeline_run\": {\n",
    "        \"variables\": {\n",
    "            \"year\": \"2023\",\n",
    "            \"month\": \"05\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.json())\n",
    "\n",
    "data[\"pipeline_run\"][\"variables\"][\"month\"] = \"06\"\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a66617e",
   "metadata": {},
   "source": [
    "![](./img/trigger_running.png)\n",
    "\n",
    "Once it is completed, we can find the predictions here `../bonus-homework/mage_project/`:\n",
    "\n",
    "![](./img/trigger_complete.png)\n",
    "\n",
    "![](./img/outputs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2677b764",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp-tracking-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
